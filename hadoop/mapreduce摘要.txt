

如果使用一个reduce实现排序，虽然可以实现排序，但失去了数据并行执行的优势；可以实现多reduce输出，并实现全排序



Applications can specify a comma separated list of paths which would be present in the current working directory of the task using the option -files. The -libjars option allows applications to add jars to the classpaths of the maps and reduces.




-- map
We’ll learn more about the number of maps spawned for a given job, and how to control them in a fine-grained manner, a bit later in the tutorial.
	The Hadoop MapReduce framework spawns one map task for each InputSplit generated by the InputFormat for the job.
    The number of maps is usually driven by the total size of the inputs, that is, the total number of blocks of the input files.


We will then discuss other core interfaces including Job, Partitioner, InputFormat, OutputFormat, and others.
Applications can then override the cleanup(Context) method to perform any required cleanup.
Applications can use the Counter to report its statistics.
Users can control the grouping by specifying a Comparator via Job.setGroupingComparatorClass(Class).


WordCount also specifies a combiner. Hence, the output of each map is passed through the local combiner (which is same as the Reducer as per the job configuration) for local aggregation, after being sorted on the keys.


The Mapper outputs are sorted and then partitioned per Reducer. The total number of partitions is the same as the number of reduce tasks for the job. Users can control which keys (and hence records) go to which Reducer by implementing a custom Partitioner.


The intermediate, sorted outputs are always stored in a simple (key-len, key, value-len, value) format. Applications can control if, and how, the intermediate outputs are to be compressed and the CompressionCodec to be used via the Configuration.
为什么会自动排序，能自定义排序吗？这个排序是全局的吗？能控制排序的规则（升降）吗？




-- reduce

The number of reduces for the job is set by the user via Job.setNumReduceTasks(int).
	实验：设置了6个，结果有6个
Applications can then override the cleanup(Context) method to perform any required cleanup.

Reducer has 3 primary phases(阶段): shuffle, sort and reduce.
	Shuffle
		Input to the Reducer is the sorted output of the mappers. In this phase the framework fetches the relevant partition of the output of all the mappers, via HTTP.

	Sort
		The framework groups Reducer inputs by keys (since different mappers may have output the same key) in this stage.

		The shuffle and sort phases occur simultaneously; while map-outputs are being fetched they are merged.

	Secondary Sort
		If equivalence rules for grouping the intermediate keys are required to be different from those for grouping keys before reduction, then one may specify a Comparator via Job.setSortComparatorClass(Class). Since Job.setGroupingComparatorClass(Class) can be used to control how intermediate keys are grouped, these can be used in conjunction to simulate secondary sort on values.

	Reduce
		In this phase the reduce(WritableComparable, Iterable<Writable>, Context) method is called for each <key, (list of values)> pair in the grouped inputs.

		The output of the reduce task is typically written to the FileSystem via Context.write(WritableComparable, Writable).

		Applications can use the Counter to report its statistics.

		The output of the Reducer is not sorted.
		# 意思是，多个reduce的话，它们产生的多个结果，不是有序的。但是如果只有一个reduce的话，那就有序了，
		  因为在此之前GroupingComparator已经做了排序
		  但是这个有序不是reducer做的哦


	Counter is a facility for MapReduce applications to report its statistics.



	More details on how to load shared libraries through distributed cache are documented at Native Libraries.

	TextInputFormat is the default InputFormat.
	TextOutputFormat is the default OutputFormat.
	所以如果map里使用了非Text的类，但是job没有对应做设置，就会导致不匹配异常
